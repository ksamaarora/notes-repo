## ğŸ§© Part 1 â€“ What is Docker

### ğŸ”¹ The Problem Weâ€™re Trying to Solve

As software developers, we build applications that millions of users depend on.
We write code, connect to databases, test features, and configure multiple services â€” all on our own local machines.

But when we share this project with teammates (developers, testers, or DevOps engineers), things often break.
Why? Because each system needs to be configured again from scratch.
If even one dependency version or OS-level setting differs, the application may fail to run.
What works on your laptop might not work in production.

To solve this problem **before containers**, we used **virtualization**.

---

### ğŸ”¹ What is Virtualization?

Virtualization introduced the idea of running multiple operating systems (OS) on a single physical machine.
It consists of three layers:

**Application â†’ OS â†’ Hardware**

And inside a virtualized setup:

* **Guest OS** â€“ the secondary OS (like Ubuntu or Windows Server) running inside the host.
* **Hypervisor** â€“ software that manages and isolates the guest OSs.
* **Host OS + Hardware** â€“ the real computer underneath.

So, in virtualization you could run two operating systems at once.
Your application runs on the **guest OS**, which runs on the **hypervisor**, which finally talks to the **host OS**.

You could even send that guest OS image to another team â€” they could copy and run it on their hypervisor.

However, to keep applications isolated, we often used multiple guest OS instances on the same server.
Each of these consumed a lot of **CPU and RAM**, even if we just wanted to run a single small app.
This made virtualization heavy and inefficient.

---

### ğŸ”¹ From Virtualization to Containerization

**Containerization** solved this.
Instead of giving every app its own guest OS, we let multiple apps share the same OS kernel but run in isolated spaces called **containers**.

A container can package everything an app needs â€” compilers, web servers, databases, and configurations â€” into one environment.
We simply give the container to any team, and it will run exactly the same everywhere.

**Benefits:**

* âœ… Isolation: Each container runs in its own isolated environment with its own filesystem, network, and dependencies. This ensures that one appâ€™s configuration or bug doesnâ€™t interfere with anotherâ€™s.

* âš¡ Lightweight (no extra OS needed): Containers share the same operating system kernel instead of running full guest OSs like in virtualization. This drastically reduces memory and CPU usage, allowing you to run many containers on the same machine.

* ğŸš€ Fast startup: Since containers donâ€™t need to boot an entire OS, they start within seconds. This helps scale systems quickly and makes development cycles faster.

* ğŸ” Secure: Containers are isolated from each other and from the host system. Even if one container is compromised, the rest remain safe, reducing security risks.

* ğŸŒ Portable across environments: A container includes all dependencies, libraries, and configurations. The same container runs identically on any system â€” local, testing, or production â€” removing the â€œit works on my machineâ€ problem.

* ğŸ“ˆ Scalable â€” you can spin up multiple identical containers easily

But how do we actually create and manage these containers?
Thatâ€™s where **Docker** comes in.

---

### ğŸ”¹ What is Docker?

**Docker** is a platform that lets us **package, distribute, and run** applications inside containers.

A **container** is a lightweight, isolated environment that includes an application along with all its dependencies, libraries, and configurations.
Because everything is self-contained, the same container runs consistently on any machine â€” whether on a laptop, testing server, or production cloud.

Before containers, every environment required separate installation and configuration for each service, often leading to dependency conflicts.
With Docker, the entire environment is defined once, and we only need one command to start it.
The only requirement on any system is the **Docker runtime** â€” which executes containers.

---

### ğŸ”¹ Where Containers Live

Containers are stored and shared through **container repositories** (registries).
They can be:

* **Public** â€” e.g. [Docker Hub](https://hub.docker.com)
* **Private** â€” internal company registries

Example:

```bash
docker run postgres:9.6
```

If the PostgreSQL 9.6 image isnâ€™t already on your machine, Docker automatically downloads it from Docker Hub.

---

### ğŸ”¹ Images and Layers

A **Docker image** is a read-only **blueprint** that defines everything needed to run an app.
Images are made up of multiple **layers**, each representing a change (like installing dependencies or adding files).

If you later pull another version (say `postgres:10`), Docker downloads only the new or different layers â€” saving both space and bandwidth.

---

### ğŸ”¹ Containers vs Images

| Concept       | Meaning                                                            |
| ------------- | ------------------------------------------------------------------ |
| **Image**     | The blueprint / artifact â€” defines what will run.                  |
| **Container** | A running instance of that image â€” the live, isolated environment. |

To view all active containers:

```bash
docker ps
```

Example output:

```
CONTAINER ID   IMAGE        COMMAND               STATUS          PORTS
fad0f8456ca7   postgres:9.6 "docker-entrypointâ€¦"  Up 47 seconds   5432/tcp
```

Here, container `fad0f8456ca7` is actively running PostgreSQL 9.6 on port 5432.

---

### ğŸ”¹ Docker Hub and Images

**Docker Hub** acts like a cloud storage for container images.
It hosts images of most popular servers, databases, and tools â€” ready to pull and run.
You can also upload (push) your own images.

To create your own image, you write a **Dockerfile**, which defines:

* The base image
* Commands to install dependencies
* Environment setup

Then you build it with:

```bash
docker build -t myapp:latest .
```

---

### ğŸ”¹ Storage and Compose

* **Docker Volumes** â€” persistent storage to save data outside containers.
* **Docker Compose** â€” a tool to run multi-container applications (e.g. web + db + cache) with a single YAML file.

---

### ğŸ§  Key Idea

Think of a **Docker image** as a **recipe**, and a **container** as the **dish** prepared from that recipe.
You can cook (run) as many dishes (containers) as you like from the same recipe (image), anywhere, anytime â€” and theyâ€™ll all taste exactly the same.
